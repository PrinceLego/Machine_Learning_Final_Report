import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
import datetime
import time
import os
import matplotlib.pyplot as plt

# === 1ï¸âƒ£ GPU åµæ¸¬ ===
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    print("âœ… GPU å·²å•Ÿç”¨ï¼š", gpus)
else:
    print("âš ï¸ æœªåµæ¸¬åˆ° GPUï¼Œå°‡ä½¿ç”¨ CPU")

# === 2ï¸âƒ£ è¼‰å…¥è³‡æ–™ ===
try:
    data = np.load('/Users/prince_lego/Documents/program/Database/wafer_Map_Datasets.npz')
    X = data['arr_0']
    y = data['arr_1']
except FileNotFoundError:
    print("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° .npz æª”æ¡ˆã€‚è«‹æª¢æŸ¥è·¯å¾‘æ˜¯å¦æ­£ç¢ºã€‚")
    exit() 

print("åŸå§‹ X shape:", X.shape)
print("åŸå§‹ y shape:", y.shape)

# === 3ï¸âƒ£ æ­£è¦åŒ–å½±åƒ ===
X = X.astype('float32') / 255.0
X = np.expand_dims(X, axis=-1)  # (N, 52, 52, 1)

# === 4ï¸âƒ£ åˆ†å‰² è¨“ç·´ / é©—è­‰ / æ¸¬è©¦é›† ===
# ç¬¬ä¸€æ¬¡åˆ†å‰²ï¼š20% æ¸¬è©¦é›†
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X, y, 
    test_size=0.2,
    random_state=42
)

# ç¬¬äºŒæ¬¡åˆ†å‰²ï¼šå‰©ä¸‹ 80% åˆ†ç‚º 60% è¨“ç·´ + 20% é©—è­‰
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, 
    test_size=0.25, # 80% * 0.25 = 20%
    random_state=42
)

# +++ Y æ¨™ç±¤æª¢æŸ¥ (ä½¿ç”¨è¨“ç·´é›†) +++
print("--- Y æ¨™ç±¤æª¢æŸ¥ (Train) ---")
print("y_train çš„å‰ 5 ç­†è³‡æ–™:\n", y_train[:5])
print("y_train çš„ç¨ç‰¹å€¼:", np.unique(y_train))
print("--------------------------")

print(f"è¨“ç·´é›†å¤§å° (Train):   {X_train.shape} (ä½” 60%)")
print(f"é©—è­‰é›†å¤§å° (Val):     {X_val.shape} (ä½” 20%)")
print(f"æ¸¬è©¦é›†å¤§å° (Test):    {X_test.shape} (ä½” 20%)")


# === 5ï¸âƒ£ å»ºç«‹ CNN æ¨¡å‹ (multi-label) ===
model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(52, 52, 1)),
    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2,2)),

    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2,2)),

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(8, activation='sigmoid')  # multi-label
])

model.summary()

# === 6ï¸âƒ£ (ä¿®æ”¹) ç·¨è­¯æ¨¡å‹ (multi-label) ===
# é™ä½å­¸ç¿’ç‡
optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)

model.compile(
    optimizer=optimizer,
    loss='binary_crossentropy',
    metrics=[
        # --- é€™è£¡ä¿®æ”¹äº† ---
        tf.keras.metrics.BinaryAccuracy(name='accuracy'), # å°‡ 'binary_acc' æ”¹åç‚º 'accuracy'
        tf.keras.metrics.AUC(name='auc')
        # --- ä¿®æ”¹çµæŸ ---
    ]
)

# === 7ï¸âƒ£ TensorBoard callback ===
log_dir = os.path.join("logs", "fit", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

# === 8ï¸âƒ£ EarlyStopping & ModelCheckpoint ===
callbacks = [
    tensorboard_cb,
    #tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
    tf.keras.callbacks.ModelCheckpoint('best_wafer_cnn_model.keras', save_best_only=True, monitor='val_auc', mode='max') 
]

# === 9ï¸âƒ£ è¨“ç·´æ¨¡å‹ ===
start_time = time.time()
history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=64,
    validation_data=(X_val, y_val),
    callbacks=callbacks,
    verbose=1
)
end_time = time.time()
print(f"è¨“ç·´ç¸½æ™‚é–“ï¼š{end_time - start_time:.2f} ç§’")

# === 1ï¸âƒ£0ï¸âƒ£ å„²å­˜æœ€çµ‚æ¨¡å‹ ===
model.save('wafer_cnn_model_final.keras')

"""
# === 1ï¸âƒ£1ï¸âƒ£ (ä¿®æ”¹) ç¹ªè£½è¨“ç·´çµæœ ===
plt.figure(figsize=(16, 6))



# --- é€™è£¡ä¿®æ”¹äº† ---
# Accuracy
plt.subplot(1, 3, 1)
plt.plot(history.history['accuracy'], label='train accuracy') # 'binary_acc' -> 'accuracy'
plt.plot(history.history['val_accuracy'], label='val accuracy') # 'val_binary_acc' -> 'val_accuracy'
plt.title('Accuracy') # 'Binary Accuracy' -> 'Accuracy'
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
# --- ä¿®æ”¹çµæŸ ---

# Loss
plt.subplot(1, 3, 2)
plt.plot(history.history['loss'], label='train loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

# AUC
plt.subplot(1, 3, 3)
plt.plot(history.history['auc'], label='train auc')
plt.plot(history.history['val_auc'], label='val auc')
plt.title('AUC')
plt.xlabel('Epoch')
plt.ylabel('AUC')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
"""


# === 1ï¸âƒ£2ï¸âƒ£ (ä¿®æ”¹) åœ¨ã€Œæ¸¬è©¦é›†ã€ä¸Šè©•ä¼°æœ€ä½³æ¨¡å‹ ===
print("\n" + "="*30)
print(" æ­£åœ¨è¼‰å…¥ã€Œæœ€ä½³æ¨¡å‹ã€é€²è¡Œæœ€çµ‚è©•ä¼°... ")
print(" (æ¨¡å‹å„²å­˜æ–¼ 'best_wafer_cnn_model.keras')")
print("="*30)

try:
    best_model = tf.keras.models.load_model('best_wafer_cnn_model.keras')
except Exception as e:
    print(f"âŒ è¼‰å…¥ 'best_wafer_cnn_model.keras' å¤±æ•—: {e}")
    print("--- å°‡ä½¿ç”¨è¨“ç·´å®Œæˆçš„ã€Œæœ€çµ‚æ¨¡å‹ã€é€²è¡Œè©•ä¼° (å¯èƒ½éæœ€ä½³) ---")
    best_model = model

print("\n--- æ­£åœ¨è©•ä¼°ã€Œæ¸¬è©¦é›†ã€(Test Set) ---")
test_results = best_model.evaluate(X_test, y_test, verbose=1)

print("\n--- ğŸš€ æœ€çµ‚æ¸¬è©¦çµæœ (Test Set) ğŸš€ ---")
# test_results åˆ—è¡¨çš„é †åºèˆ‡ model.compile ä¸­çš„ metrics ç›¸åŒ
print(f"  æ¸¬è©¦é›† Loss:            {test_results[0]:.4f}")
# --- é€™è£¡ä¿®æ”¹äº† ---
print(f"  æ¸¬è©¦é›† Accuracy:        {test_results[1]:.4f}") # 'Binary Accuracy' -> 'Accuracy'
# --- ä¿®æ”¹çµæŸ ---
print(f"  æ¸¬è©¦é›† AUC:             {test_results[2]:.4f}")
print("="*38)